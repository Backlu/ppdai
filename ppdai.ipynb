{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PPDAI Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ppdaiutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'TRAIN_PATH':'data/train.csv',\n",
    "    'TEST_PATH':'data/test.csv',\n",
    "    'QUESTION_PATH' : 'data/question.csv',   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** read data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Load files...')\n",
    "data={\n",
    "    'qes' : pd.read_csv(config['QUESTION_PATH']),\n",
    "    'tr' : pd.read_csv(config['TRAIN_PATH']),\n",
    "    'te' : pd.read_csv(config['TEST_PATH']),\n",
    "    #'co' : questions['words'],\n",
    "}\n",
    "data['co']=data['qes']['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    display(data['qes'].head())\n",
    "    display(data['tr'].head())\n",
    "    display(data['te'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ID轉成詞語序列or單字序列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(qids):\n",
    "    ids = []\n",
    "    for t_ in qids:\n",
    "        ids.append(int(t_[1:]))\n",
    "    return np.asarray(ids)\n",
    "\n",
    "def get_textschars(d):\n",
    "    all_words = data['qes']['words']\n",
    "    all_chars = data['qes']['chars']\n",
    "    q1id, q2id = d['q1'], d['q2']\n",
    "    id1s, id2s = get_ids(q1id), get_ids(q2id)\n",
    "    q1_texts = []\n",
    "    q2_texts = []\n",
    "    for t_ in zip(id1s, id2s):\n",
    "        q1_texts.append(all_words[t_[0]])\n",
    "        q2_texts.append(all_words[t_[1]])\n",
    "    d['q1_texts'] = q1_texts\n",
    "    d['q2_texts'] = q2_texts\n",
    "    \n",
    "    q1_chars = []\n",
    "    q2_chars = []\n",
    "    for t_ in zip(id1s, id2s):\n",
    "        q1_chars.append(all_chars[t_[0]])\n",
    "        q2_chars.append(all_chars[t_[1]])\n",
    "    d['q1_chars'] = q1_chars\n",
    "    d['q2_chars'] = q2_chars\n",
    "    \n",
    "\n",
    "print('Get texts/chars...')\n",
    "get_textschars(data['tr'])\n",
    "get_textschars(data['te'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tr'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 序列化**\n",
    "- tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('data/word_embed.txt') as f:\n",
    "    MAX_NB_WORDS = (len(list(f)))\n",
    "\n",
    "trq1_text=data['tr']['q1_texts'].values\n",
    "trq2_text=data['tr']['q2_texts'].values\n",
    "teq1_text=data['te']['q1_texts'].values\n",
    "teq2_text=data['te']['q2_texts'].values\n",
    "alltext=np.concatenate([trq1_text, trq2_text, teq1_text, teq2_text])\n",
    "MAX_SEQUENCE_LENGTH = max(list(map(lambda x: len(x), alltext))) \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "tokenizer.fit_on_texts(alltext) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['tr']['q1_sequences'] = tokenizer.texts_to_sequences(trq1_text) \n",
    "data['tr']['q2_sequences'] = tokenizer.texts_to_sequences(trq2_text) \n",
    "data['te']['q1_sequences'] = tokenizer.texts_to_sequences(teq1_text) \n",
    "data['te']['q2_sequences'] = tokenizer.texts_to_sequences(teq2_text) \n",
    "#data['tr'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index \n",
    "print('Found %s unique tokens' % len(word_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. pad_sequences **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['trq1_padseq'] = pad_sequences(data['tr']['q1_sequences'], maxlen=MAX_SEQUENCE_LENGTH) \n",
    "data['trq2_padseq'] = pad_sequences(data['tr']['q2_sequences'], maxlen=MAX_SEQUENCE_LENGTH) \n",
    "data['teq1_padseq'] = pad_sequences(data['te']['q1_sequences'], maxlen=MAX_SEQUENCE_LENGTH) \n",
    "data['teq2_padseq'] = pad_sequences(data['te']['q2_sequences'], maxlen=MAX_SEQUENCE_LENGTH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. prepare embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_FILE='data/word_embed.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embeddings_index = {} \n",
    "f = open(EMBEDDING_FILE,\"rb\") \n",
    "for line in f: \n",
    "    values = line.split() \n",
    "    word = values[0].decode(encoding='utf-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32') \n",
    "    embeddings_index[word] = coefs \n",
    "f.close() \n",
    "\n",
    "nb_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM)) \n",
    "for word, i in word_index.items(): \n",
    "    word = word.upper()\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** prepare training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample train/validation data\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "trlen = len(data['trq1_padseq'])\n",
    "perm = np.random.permutation(trlen)\n",
    "idx_train = perm[:int(trlen*(1-VALIDATION_SPLIT))] \n",
    "idx_val = perm[int(trlen*(1-VALIDATION_SPLIT)):] \n",
    "\n",
    "data_trainq1=data['trq1_padseq'][idx_train] \n",
    "data_trainq2=data['trq2_padseq'][idx_train] \n",
    "data_valq1=data['trq1_padseq'][idx_val] \n",
    "data_valq2=data['trq2_padseq'][idx_val] \n",
    "\n",
    "labels_train = data['tr']['label'][idx_train] \n",
    "labels_val = data['tr']['label'][idx_val] \n",
    "\n",
    "\n",
    "date_testq1 = data['teq1_padseq']\n",
    "date_testq2 = data['teq2_padseq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=nb_words, output_dim=300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False) \n",
    "num_lstm = 300 \n",
    "num_dense = 256 \n",
    "rate_drop_lstm = 0.25 \n",
    "rate_drop_dense = 0.25 \n",
    "act = 'relu' \n",
    "\n",
    "\n",
    "q1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='q1_input') \n",
    "q2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='q2_input') \n",
    "q1_embseq= embedding_layer(q1_input) \n",
    "q2_embseq= embedding_layer(q2_input) \n",
    "\n",
    "lstm_layerq1 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True, name='q1_lstm') \n",
    "q1_lstm = lstm_layerq1(q1_embseq) \n",
    "q1_drop = Dropout(rate_drop_dense, name='q1_drop')(q1_lstm) \n",
    "q1_att = Attention(MAX_SEQUENCE_LENGTH)(q1_drop)\n",
    "\n",
    "lstm_layerq2 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True, name='q2_lstm') \n",
    "q2_lstm = lstm_layerq2(q2_embseq) \n",
    "q2_drop = Dropout(rate_drop_dense, name='q2_drop')(q2_lstm) \n",
    "q2_att = Attention(MAX_SEQUENCE_LENGTH)(q2_drop)\n",
    "\n",
    "q1q2_concat = Concatenate(axis=-1,name='q1q2concat')([q1_att,q2_att])\n",
    "q1q2_concat = Dense(num_dense, activation=act, name='Q_dense')(q1q2_concat) \n",
    "q1q2_concat = Dropout(rate_drop_dense, name='Q_drop')(q1q2_concat) \n",
    "q1q2_concat = BatchNormalization(name='Q_batchnorm')(q1q2_concat) \n",
    "preds = Dense(1, activation='sigmoid', name='Q_output')(q1q2_concat)\n",
    "\n",
    "model = Model(inputs=[q1_input, q2_input],  outputs=preds) \n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "print(model.summary()) \n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** training **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STAMP = 'model/simple_lstm_glove_vectors_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense) \n",
    "print('STAMP',STAMP)\n",
    "bst_model_path = STAMP + '.h5' \n",
    "print('bst_model_path',bst_model_path) \n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5) \n",
    "#model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True) \n",
    "\n",
    "hist = model.fit([data_trainq1, data_trainq2], labels_train, validation_data=([data_valq1,data_valq2], labels_val), epochs=50, batch_size=256, shuffle=True, callbacks=[early_stopping]) \n",
    "\n",
    "#model.load_weights(bst_model_path) \n",
    "bst_val_score = min(hist.history['val_loss']) \n",
    "\n",
    "y_test = model.predict([date_testq1, date_testq2], batch_size=1024, verbose=1) \n",
    "\n",
    "#data['sam'][list_classes] = y_test \n",
    "#data['sam'].to_csv('%.4f_'%(bst_val_score) + STAMP + '.csv', index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_submission(predict_prob):\n",
    "    with open('submission.csv', 'w') as file:\n",
    "        file.write(str('y_pre') + '\\n')\n",
    "        for line in predict_prob:\n",
    "            file.write(str(line) + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "testpred = model.predict([date_testq1, date_testq2], batch_size=1024, verbose=1) \n",
    "make_submission(testpred[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TOTDO: \n",
    "1. 是否成改成logloss\n",
    "2. 確認test data的logloss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
